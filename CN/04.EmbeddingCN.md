# **第四章. 透過語義嵌入 Embeddings 來調教你的 OpenAI 模型**

很多行業希望擁有 OpenAI 的能力，希望 OpenAI 能解決自己的企業內部問題。這就包括員工相關的內容如入職須知，請假和報銷流程，還有福利查詢等，企業業務流相關的內容包括相關文件，法規，執行流程等，也有一些面向客戶的查詢。雖然 OpenAI 有強大的知識能力，但是基於行業的資料和知識是沒辦法獲取的。那如何注入這些基於行業的知識內容呢？這也是讓 OpenAI 邁入企業化重要的一步。本章我們就會和大家講講如何注入行業的資料和知識，讓 OpenAI 變得更專業。

## **4.1. 從自然語言中的向量談起**

在自然語言領域，我們知道最細的粒度是詞，片語成句，句構成段落，篇章和最後的文件。計算機是不認識詞的，所以我們需要對詞轉換為數學上的表示。這個表示就是向量，也就是 Vector。向量是一個數學上的概念，它是一個有方向的量，有大小和方向。 有了向量，我們可以有效地對文字進行向量化，這也是計算機自然語言領域的基礎。在自然語言處理領域，我們有很多向量化的方法，比如 One-hot，TF-IDF，Word2Vec，Glove，ELMO，GPT，BERT 等。這些向量化的方法都有各自的優缺點，但是都是基於詞的向量化，也就是詞向量。詞向量是自然語言處理的基礎，也是 OpenAI 所有模型的基礎。我們分別看看詞向量裡面的幾種常見方法。

### **4.1.1 One-hot 編碼**

One-hot 編碼，是用 0 和 1 的編碼方式來表示詞。比如我們有 4 個詞，分別是：`我`，`愛`，`北京`，`天安門`。那麼我們可以用 4 個向量來表示這 4 個詞，分別是：

```

我 = [1, 0, 0, 0]
愛 = [0, 1, 0, 0]
北京 = [0, 0, 1, 0]
天安門 = [0, 0, 0, 1]

```

在傳統的自然語言應用場景中，我們把每個詞看成用 One-Hot 向量表示，作為唯一的離散符號。我們的詞庫中有單詞的數量就是向量的維度，如上述的例子總共包含了四個詞，所以我們可以用一個四維的向量來表示。在這個向量中，每個詞都是唯一的，也就是說每個詞都是獨立的，沒有任何關係。這樣的向量我們稱為 One-Hot 向量。One-Hot 向量的優點是簡單，容易理解，而且每個詞都是唯一的，沒有任何關係。但是 One-Hot 向量的缺點也很明顯，就是向量的維度會隨著詞的增加而增加。比如我們有 1000 個詞，那麼我們的向量就是 1000 維的。這樣的向量是非常稀疏的，也就是大部分的值都是 0。這樣的向量會導致計算機的計算量非常大，而且也不利於計算機的計算。所以 One-Hot 編碼的缺點就是向量維度大，計算量大，計算效率低。

### **4.1.2. TF-IDF 編碼**

TF-IDF 是一個統計學，透過評估一個詞對一個語料的重要程度。TF-IDF 是 Term Frequency - Inverse Document Frequency 的縮寫，中文叫做詞頻-逆文件頻率。TF-IDF 的主要思想是：如果某個詞在一篇文章中出現的頻率高，並且在其他文章中很少出現，那麼這個詞就是這篇文章的關鍵詞。一般我們習慣把這個概念拆分，分為 TF 和 IDF 兩個部分。

**TF - 詞頻**

詞頻（Term Frequency）指的是某個詞在文章中出現的頻率。詞頻的計算公式如下：

```

TF = 某個詞在文章中出現的次數 / 文章的總詞數

```

TF 有一個問題，就是如果一個詞在文章中出現的次數很多，那麼這個詞的 TF 值就會很大。這樣的話，我們就會認為這個詞是這篇文章的關鍵詞。但是這樣的話，我們會發現，很多詞都是這篇文章的關鍵詞，這樣的話，我們就無法區分哪些詞是這篇文章的關鍵詞了。所以我們需要對 TF 進行一些調整，這個調整就是 IDF。

**IDF - 逆文件頻率**

逆文件頻率（Inverse Document Frequency）指的是某個詞在所有文章中出現的頻率。逆文件頻率的計算公式如下：

```

IDF = log(語料庫的文件總數 / (包含該詞的文件數 + 1))

```

IDF 的計算公式中，分母加 1 是為了避免分母為 0 的情況。IDF 的計算公式中，語料庫的文件總數是固定的，所以我們只需要計算包含該詞的文件數就可以了。如果一個詞在很多文章中都出現，那麼這個詞的 IDF 值就會很小。如果一個詞在很少的文章中出現，那麼這個詞的 IDF 值就會很大。這樣的話，我們就可以透過 TF 和 IDF 的乘積來計算一個詞的 TF-IDF 值。TF-IDF 的計算公式如下：

```

TF-IDF = TF * IDF

```

TF-IDF 經常用於文字分類別的場景，這也是 TF-IDF 最常用的場景。TF-IDF 的優點是簡單，容易理解，而且計算量也不大。TF-IDF 的缺點是沒有考慮詞的順序，而且沒有考慮詞與詞之間的關係。所以 TF-IDF 適合用於文字分類別的場景，而不適合用於文字產生的場景。


### **4.1.3. Word2Vec 編碼**

Word2Vec 我們也叫它做 Word Embeddings，中文叫做詞嵌入。Word2Vec 的主要思想是：一個詞的語義可以透過它的上下文來確定。Word2Vec 有兩種模型，分別是 CBOW 和 Skip-Gram。CBOW 是 Continuous Bag-of-Words 的縮寫，中文叫做連續詞袋模型。Skip-Gram 是 Skip-Gram Model 的縮寫，中文叫做跳字模型。CBOW 模型的思想是透過一個詞的上下文來預測這個詞。Skip-Gram 模型的思想是透過一個詞來預測這個詞的上下文。Word2Vec 的優點是可以得到詞的語義，而且可以得到詞與詞之間的關係。對比起 One-Hot 編碼和 TF-IDF 編碼，Word2Vec 編碼的優點是可以得到詞的語義，而且可以得到詞與詞之間的關係。Word2Vec 編碼的缺點是計算量大，而且需要大量的語料庫。

之前我們提及過，One-Hot 編碼的維度是詞的個數，而 Word2Vec 編碼的維度是可以指定的。一般我們會指定為 100 維或者 300 維。Word2Vec 編碼的維度越高，詞與詞之間的關係就越豐富，但是計算量也就越大。Word2Vec 編碼的維度越低，詞與詞之間的關係就越簡單，但是計算量也就越小。Word2Vec 編碼的維度一般是 100 維或者 300 維，這樣的維度可以滿足大部分的應用場景。


Word2Vec 編碼的計算公式非常簡單，就是 Word Embeddings。Word Embeddings 是一個詞向量，它的維度是可以指定的。Word Embeddings 的維度一般是 100 維或者 300 維，這樣的維度可以滿足大部分的應用場景。Word Embeddings 的計算公式如下：

``` 

Word Embeddings = 詞的語義 + 詞與詞之間的關係

```

可以把 Word2Vec 看作是簡單化的神經網路。


### **4.1.4. GPT 模型**

GPT 模型的全稱是 Generative Pre-Training，中文叫做預訓練產生模型。GPT 模型是 OpenAI 在 2018 年提出的，它的主要思想是：一個詞的語義可以透過它的上下文來確定。GPT 模型的優點是可以得到詞的語義，而且可以得到詞與詞之間的關係。GPT 模型的缺點是計算量大，而且需要大量的語料庫。GPT 模型的結構是一個多層單向的 Transformer 結構，它的結構如下圖所示：


<img src="./../imgs/04/GPT.png">


訓練過程是兩個階段，第一個階段是預訓練，第二個階段是微調。預訓練的語料是維基百科和 BookCorpus，微調的語料是不同的自然語言任務。預訓練的目標是透過一個詞的上下文來預測這個詞，微調的目標是根據不同的自然語言任務，如文字分類、文字產生、問答系統等，對語義模型進行微調，得到不同的模型。

GPT 模型已經經歷了 4 個階段，最為出名的就是 ChatGPT 所使用的 GPT-3.5 以及 GPT 4 。GPT 開啟了全新的時代，它的出現讓我們看到了自然語言處理的無限可能。GPT 模型的優點是可以得到詞的語義，而且可以得到詞與詞之間的關係。GPT 模型的缺點是計算量大，而且需要大量的語料庫。很多人希望擁有自己行業對標的 GPT ，這也是我們本章需要解決的問題。


### **4.1.5. BERT 編碼**

BERT 是 Bidirectional Encoder Representations from Transformers 的縮寫，中文叫做雙向編碼器的 Transformer。BERT 是一個預訓練的模型，它的訓練語料是維基百科和 BookCorpus。BERT 的主要思想是：一個詞的語義可以透過它的上下文來確定。BERT 的優點是可以得到詞的語義，而且可以得到詞與詞之間的關係。對比起 One-Hot 編碼、TF-IDF 編碼和 Word2Vec 編碼，BERT 編碼的優點是可以得到詞的語義，而且可以得到詞與詞之間的關係。BERT 編碼的缺點是計算量大，而且需要大量的語料庫。


## **4.2 Embeddings 嵌入技術**

在第一節中我們提及了 One-Hot 編碼、TF-IDF 編碼、Word2Vec 編碼、BERT 編碼和 GPT 模型。這些編碼和模型都是 Embeddings 嵌入技術的一種。Embeddings 嵌入技術的主要思想是：一個詞的語義可以透過它的上下文來確定。Embeddings 嵌入技術的優點是可以得到詞的語義，而且可以得到詞與詞之間的關係。Embeddings 是作為自然語言深度學習的基礎，它的出現讓我們看到了自然語言處理的無限可能。

對於文字內容的 Embeddings 方法，我們結合上一節，你會發現從 word2vec 技術誕生後，文字內容的 Embeddings 就不斷得到加強，從 word2vec 到 GPT 再到 BERT ,Embeddings 技術的效果越來越好 。Embeddings 技術的本質就是“壓縮”，用更少的維度來表示更多的資訊。這樣的好處是可以節省儲存空間，提高計算效率。

在 OpenAI 中，Embeddings 技術的應用非常廣泛，將文字字串轉換為浮點向量，透過向量之間的距離來衡量文字之間的相似度。不同行業希望加入自己的資料 我們就可以把這些企業級的資料透過 OpenAI Embeddings - text-embedding-ada-002 模型查詢出向量，並透過對映進行儲存，在使用時將問題也轉換為向量，透過相似度的演算法對比，找出最接近的 TopN 結果，從而找到與問題相關聯的企業內容。

我們可以透過向量資料庫將企業資料向量化後儲存，結合 text-embedding-ada-002模型透過向量的相似度進行查詢，從而找到與問題相關聯的企業內容。現在常用的向量資料庫就包括 Qdrant, Milvus, Faiss, Annoy, NMSLIB 等。


### **4.2.1 Open AI 的 Embeddings 模型**

OpenAI 的文字嵌入向量文字字串的相關性。 嵌入通常用於以下場景

1. 搜尋（結果按與查詢字串的相關性排序）
2. 聚類（其中文字字串按相似性分組）
3. 推薦（推薦具有相關文字字串的專案）
4. 例外檢測（識別出相關性很小的例外值）
5. 多樣性測量（分析相似性分佈）
6. 分類（其中文字字串按其最相似的標籤分類）

嵌入是浮點數的向量（列表）。 兩個向量之間的距離衡量它們的相關性。 小距離表示高相關性，大距離表示低相關性。 例如，如果您有一個嵌入為[0.1,0.2,0.3]的字串“狗”，則該字串與嵌入為[0.2,0.3,0.4]的字串“貓”比與嵌入為[0.9,0.8,0.7]的字串“汽車”更相關。

我們可以嘗試引入一些時效性的問題來測試下 OpenAI 的 Embeddings 模型的效果。來看看例子

**案例** -  我們在 ChatGPT 中經常會問一些時效性的問題，例如以下場景

<img src="./../imgs/04/01.png">

這個答案對嗎？這是一個錯誤的答案，因為在 2023 年 5 月 3 日凌晨阿森納是 3:1 戰勝了切爾西。這是一個錯誤的結果。產生這個錯誤是因為 ChatGPT 不具備時效性。這個時候我們就需要給 GPT 一些時效性的資訊，我們可以結合之前說的 Bing Search 來完成相關材料的補充。透過這些資料透過向量化去配對問題。(如果你不知道如何使用 Bing Search , 請參考第三章的內容)。因為這是搜尋結果，所以這裡我希望啟動兩個模型 text-search-curie-doc-001 來配對並進行轉化。找到與問題和內容最相近的前三個作為樣本嵌入，來看看效果。

<img src="./../imgs/04/02.png">

我們可以透過 Bing 搜尋找到最佳的 5 個答案

<img src="./../imgs/04/03.png">

接下來我們在 Azure OpenAI Portal 部署 text-search-curie-doc-001 

<img src="./../imgs/04/04.png">

```csharp

int n = 0 ;
foreach(var item in docs)
{
    var contentString = JsonSerializer.Serialize(item);
    var content = new StringContent(contentString, Encoding.UTF8, "application/json");
    var result = await client.PostAsync(EMBEDDING_URL, content);
    var result_string = await result.Content.ReadAsStringAsync();
    var vector = JsonDocument.Parse(result_string).RootElement.GetProperty("data").EnumerateArray().First().GetProperty("embedding").EnumerateArray().Select(x => x.GetDouble()).ToList<double>();
    var query = new QueryEmbeddingPrompt
    {
        title = newsList[n].name,
        content = newsList[n].description,
        input = vector
    };
    queries.Add(query);
    n++;
}


```

我們也需要把問題 "2023年5月3日阿森納與切爾西的賽果" 做向量轉化

<img src="./../imgs/04/05.png">

我們可以透過 NumSharp 把問題和找到內容進行相似性對映，以找到最佳的三個答案，作為少樣本。

<img src="./../imgs/04/06.png">

然後就可以作為少樣本進行提示，當再問一次“5月3日阿森納與切爾西的賽果”就會有相關內容給出


<img src="./../imgs/04/07.png">


如果你想了解更多可以參考 ./Code/04.Embeddings/01.EmbeddingsWithBing.ipynb

這個是我們把時效內容向量化後配對合適的內容作為少樣本的案例，實際上我們也可以把這些準確的語料儲存。這個時候我們就需要向量資料庫來解決。


### **4.2.2 向量資料庫 - Qdrant**

隨著 OpenAI 有更多企業級別的落地，向量資料庫也越來越受到關注。用好向量資料庫，可以讓我們的業務更加高效。向量資料庫的主要功能是將向量資料進行儲存和查詢。向量資料庫的優點是可以快速查詢，結合 OpenAI Embeddings 模型 - text-embedding-ada-002 可以打造自身企業的知識圖譜，從而提高企業的效率。


<img src="./../imgs/04/Qdrant.png">

Qdrant（讀作：quadrant）是一個向量相似度搜索引擎。 它提供了一個生產就緒的服務，帶有一個方便的 API 來儲存、搜尋和管理點——帶有額外有效負載的向量。 Qdrant 專為擴充過濾支援而客製。 它使它可用於各種神經網路或基於語義的匹配、分面搜尋和其他應用程式。

Semantic Kernel 支援 Qdrant 的呼叫，我們可以結合 Semantic Kernel 和 Qdrant 來實現知識圖譜。使用方式如下：

1. 你必須安裝 Docker https://www.docker.com/products/docker-desktop/ 

2. 完成本地 Qdrant 的安裝


```bash

docker pull qdrant/qdrant

```

   安裝完後，啟動 Qdrant


```bash

docker run -p 6333:6333 \
    -v $(pwd)/path/to/data:/qdrant/storage \
    qdrant/qdrant

```


3. 開啟 ./Code/04.Embeddings/02.SKQdrantDemo.ipynb ，執行


<img src="./../imgs/04/08.png">


透過向量資料庫你可以嵌入不同的專業資料，我們會在最後一章介紹更多的內容。

## **4.3 Embeddings vs Fine-Tuning**

 透過 Embeddings 語義嵌入或者 Fine-Tuning 微調對大規模語言模型 (GPT-3/3.5/4) 進行使用，是受人矚目的技術話題。雖然 Embeddings 語義嵌入和微調都是可以注入大規模語言模型的不同註釋和語義的技術，但是它們的應用場景和使用方式是不同的。利用本節和大家說說 Embeddings 嵌入和微調的區別和使用場景。


### **4.3.1 從概念說起**

**語義嵌入** - 如上面提及的語義嵌入是文字的向量表示，可捕獲單詞或短語的語義。 透過比較和分析這些向量，可以辨別文字元素之間的異同。

利用語義嵌入進行搜尋可以快速有效地檢索相關資訊，尤其是在大型資料集中。 與微調相比，語義搜尋具有多項優勢，例如更快的搜尋速度、更低的計算成本以及防止虛構或事實捏造。 由於這些好處，當目標是存取模型中的特定知識時，語義搜尋通常受到青睞。嵌入在各個領域都有應用，包括推薦引擎、搜尋功能和文字分類別。 

在語義嵌入中，我們會遇到以下的挑戰

1. 高維度而增加的複雜性 - 大型語言模型產生的嵌入通常具有高維，這會導致計算複雜性和儲存需求增加。 這使得執行時變得複雜。
   
2. 向量出現稀疏矩陣 - 嵌入可能導致稀疏表示，其中向量中的大多數元素都是零或接近零的值。 這種稀疏性會導致相似性搜尋或其他任務期間記憶體消耗增加和計算時間變慢。
   
3. 可解釋性 - 嵌入通常難以解釋，因為它們表示高維空間中的複雜關係。 這種可解釋性的缺乏使得診斷問題（例如嵌入中的偏差或不準確）以及理解模型的潛在推理變得具有挑戰性。
   
4. 資料品質 - 嵌入的有效性在很大程度上取決於用於訓練大型語言模型的輸入資料品質。不正確的資料可能導致嵌入效果不理想，
   
5. 專業性 - 雖然預訓練的 LLM 可以產生通用嵌入，但將這些嵌入適應特定領域或任務可能需要對特定領域資料進行額外的微調或訓練。 此過程可能會佔用大量資源，並且需要模型訓練和最佳化方面的專業知識。

6. 道德：嵌入可能會無意中捕獲並延續訓練資料中存在的偏見，例如性別、種族或文化偏見。 然後，這些偏差會影響模型在下游任務中的行為，引發對公平性和倫理影響的擔憂。


**微調** - 微調是一種用於改進預訓練模型（例如聊天機器人）效能的技術。 透過提供範例和調整模型的引數，微調使模型能夠為特定任務產生更準確和上下文相關的響應。 這些任務的範圍從聊天機器人對話和程式碼產生器到問題形成，確保更好地與所需輸出保持一致。 該過程類似於神經網路在訓練期間調整其權重。作為遷移學習的一種形式，微調可以調整預訓練模型來執行新任務，而無需進行大量的再訓練。 該過程涉及對模型的引數進行微調，使其能夠更好地執行目標任務。

對 GPT-3 等大型語言模型 (LLM) 進行微調會帶來一些挑戰，這些挑戰可能會影響其效率、可擴充性和有效性。 這就包括：

1. 算力成本 - 微調 LLM 需要大量計算資源，這可能很昂貴，而且對於預算有限的組織或研究人員來說可能不可行。 這些成本會限制微調在使用中的可擴充性。

2. 語料品質問題 - 高品質和相關的語料資料對於成功進行微調至關重要。 高品質的語料從自然語言開始就是很難找到的, 引入品質差的語料可能影響模型效能的偏差或不準確的風險。胡說八道的機會增加。
   
3. 過度擬合 - 過度擬合發生在模型對訓練資料過於專業化時，導致對新範例的泛化能力較差。 如何在專業化和泛化之間的平衡對於實現最佳效能至關重要。
   
4. 胡說八道：微調 LLM 有時會導致虛構，即模型產生不正確或捏造的資訊，以及幻覺，即它產生似是而非但不正確的答案。 這可能會破壞模型輸出的可靠性和可信度。
   
5. 模型適應性：當新資訊或更新知識可用時，可能需要重新微調模型。 此過程可能會佔用大量資源且繁瑣，尤其是在需要經常更新模型以保持最新和相關性的情況下。
 
6. 道德：微調 LLM 可能會導致潛在的偏見，因為它們可能會無意中從訓練資料中學習和傳播有害的刻板印象或錯誤資訊。 確保微調模型的道德使用和輸出可能具有挑戰性，需要不斷監測和評估。


### **4.3.2 如何選擇**

語義嵌入和微調是利用 GPT-3 等語言模型的兩個主要方法。 微調專注於透過遷移學習教授模型新任務，而語義嵌入涉及將文字的含義轉換為向量表示，可用於語義搜尋和資訊檢索等任務。

語義搜尋，也稱為神經搜尋或向量搜尋，使資料庫能夠根據語義而不是僅根據關鍵字或索引來搜尋和檢索資訊。 這種方法具有高度可擴充性，通常比針對特定任務微調模型更快且更具成本效益。

相比之下，微調可能非常耗時、複雜且成本高昂，並且不會從本質上增強模型儲存和檢索新資訊的能力。 但透過微調的資料，很多時候不能的出正確的效果，依然會胡說八道。在使用嵌入和微調問答之間的選擇取決於具體要求和問題的複雜性。 對於更簡單的搜尋任務，嵌入可以更有效且更容易實現，而對於更復雜的問答場景，微調可能會提供更好的結果。


## **4.4 本章小結**

本章我們介紹了語義嵌入 Embeddings 來調教你的模型，讓你能注入行業應用資料，讓模型更加專業化。在引入前，我們還把以往自然語言的處理方法做了一個簡單的回顧，讓你能更好的理解語義嵌入的優勢。當然我們也比較了語義嵌入和微調的優缺點，讓你能更好的選擇適合你的場景。希望透過賁張的學習你能有一個更好的理解。


### **相關資料**

1. 關於語義嵌入 Embeddings https://learn.microsoft.com/zh-cn/azure/cognitive-services/openai/tutorials/embeddings

2. 關於 Qdrant https://qdrant.tech/






















